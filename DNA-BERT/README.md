# DNA-BERT
According to [1], there are four pre-trained Bert-based DNA models, 3-new-12w-0, 4-new-12w-0, 5-new-12w-0 and 6-new-12w-0, respectively.
Please download the pre-trained Bert-based DNA models from the this [link](https://drive.google.com/drive/folders/1qzvCzYbx0UIZV3HY4pEEeIm3d_mqZRcb?usp=sharing). 

or download from the following links:
* [6-new-12w-0](https://drive.google.com/drive/folders/1EdUXFvBBaC9TMidDnJPDa7un2-yvsLq9?usp=sharing)
* [5-new-12w-0](https://drive.google.com/drive/folders/1EMBTf1b1mJSJhZNKmQXmGYS9nXdv6_ik?usp=sharing)
* [4-new-12w-0](https://drive.google.com/drive/folders/1hFKekXro-ygIUvyjxGI9kTW-hKu_v8BS?usp=sharing)
* [3-new-12w-0](https://drive.google.com/drive/folders/1raCeWXrfebtjSrgkgpBHPu0jTOP5uFRG?usp=sharing)


# Reference
[1] Ji Y, Zhou Z, Liu H, et al. DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome[J]. Bioinformatics, 2021, 37(15): 2112-2120.
