{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiahao/anaconda3/envs/tensorflow_pytorch_python/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import * \n",
    "from model import *\n",
    "from pandas import DataFrame\n",
    "from sklearn import metrics\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epoches',              type=int,  default=30,  help='')\n",
    "parser.add_argument('--batch_size',           type=int,  default=128,  help='')\n",
    "parser.add_argument('--max_length',           type=int,  default=200, help='')\n",
    "parser.add_argument('--learning_rate',        type=float, default=1e-4, help=\"\")\n",
    "parser.add_argument('--model_path',           type=str,  default=\"../3-new-12w-0\", help='')\n",
    "parser.add_argument('--ind_filename',  type=str,  default=\"../dataset/enhancer_3-mer_DNABERT_ind.txt\", help='')\n",
    "parser.add_argument('--tra_filename',  type=str,  default=\"../dataset/enhancer_3-mer_DNABERT_tra.txt\", help='')\n",
    "\n",
    "args = parser.parse_args(args=[]) # 如果不使用\"args=[]\"，会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(model, dataloader, mer):\n",
    "    output_embeddings = []\n",
    "    output_labels = []\n",
    "    for batch_data in dataloader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():   # No gradient\n",
    "            input_ids = batch_data[\"input_ids\"].to(device)\n",
    "            attention_mask = batch_data[\"attention_mask\"].to(device)\n",
    "            labels = batch_data[\"labels\"]\n",
    "            \n",
    "            embeddings = model(input_ids, attention_mask, kmer=mer).to(\"cpu\").numpy()\n",
    "        \n",
    "        if len(output_embeddings) == 0:\n",
    "            output_embeddings = embeddings\n",
    "            output_labels = labels\n",
    "        else:\n",
    "            output_embeddings = np.concatenate((output_embeddings, embeddings), axis=0)\n",
    "            output_labels = np.concatenate((output_labels, labels), axis=0)\n",
    "\n",
    "    return output_embeddings, output_labels\n",
    "\n",
    "\n",
    "def train_classifier(model, dataloader, optimizer):\n",
    "    train_iter, train_loss_sum = 0.0, 0.0\n",
    "    real_labels, pre_labels = [], []\n",
    "    for batch_data in dataloader:\n",
    "        batch_features = batch_data[\"features\"].to(device)\n",
    "        batch_labels = batch_data[\"labels\"].to(device)\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features, batch_labels)\n",
    "        \n",
    "        # 反向梯度信息\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute performance\n",
    "        train_loss = outputs[0].detach().to(\"cpu\").numpy()\n",
    "        logits = outputs[1].detach()\n",
    "\n",
    "        train_iter += 1\n",
    "        train_loss_sum += train_loss\n",
    "\n",
    "        if len(real_labels) == 0:\n",
    "            real_labels = batch_labels.to(\"cpu\").numpy()\n",
    "            pre_labels = logits.to(\"cpu\").numpy()\n",
    "        else:\n",
    "            real_labels = np.concatenate([real_labels, batch_labels.to(\"cpu\").numpy()], axis=0)\n",
    "            pre_labels = np.concatenate([pre_labels, logits.to(\"cpu\").numpy()], axis=0)\n",
    "\n",
    "    tra_loss = train_loss_sum/(train_iter)\n",
    "    acc, mcc, sn, sp = evaluation_criterion(pre_labels, real_labels)\n",
    "\n",
    "    fpr, tpr, threshold = metrics.roc_curve(real_labels, pre_labels)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return tra_loss, acc, mcc, sn, sp, roc_auc\n",
    "\n",
    "\n",
    "def test_classifier(model, dataloader, optimizer):\n",
    "    test_iter, test_loss_sum = 0.0, 0.0\n",
    "    real_labels, pre_labels = [], []\n",
    "    for batch_data in dataloader:\n",
    "        batch_features = batch_data[\"features\"].to(device)\n",
    "        batch_labels = batch_data[\"labels\"].to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_features, batch_labels)\n",
    "        \n",
    "        # compute performance\n",
    "        test_loss = outputs[0].detach().to(\"cpu\").numpy()\n",
    "        logits = outputs[1].detach()\n",
    "\n",
    "        test_iter += 1\n",
    "        test_loss_sum += test_loss\n",
    "\n",
    "        if len(real_labels) == 0:\n",
    "            real_labels = batch_labels.to(\"cpu\").numpy()\n",
    "            pre_labels = logits.to(\"cpu\").numpy()\n",
    "        else:\n",
    "            real_labels = np.concatenate([real_labels, batch_labels.to(\"cpu\").numpy()], axis=0)\n",
    "            pre_labels = np.concatenate([pre_labels, logits.to(\"cpu\").numpy()], axis=0)\n",
    "\n",
    "    test_loss = test_loss_sum/(test_iter)\n",
    "    acc, mcc, sn, sp = evaluation_criterion(pre_labels, real_labels)\n",
    "\n",
    "    fpr, tpr, threshold = metrics.roc_curve(real_labels, pre_labels)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return test_loss, acc, mcc, sn, sp, roc_auc, real_labels, pre_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../3-new-12w-0 were not used when initializing C_Bert_average_embedding: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing C_Bert_average_embedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing C_Bert_average_embedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; train loss:0.65494, acc: 0.70788, mcc: 0.42572, sn: 0.60040, sp: 0.81536, auc: 0.78083; test loss: 0.63966, acc: 0.70000, mcc: 0.44791, sn: 0.47500, sp: 0.92500, auc: 0.82515;\n",
      "\n",
      "epoch: 1; train loss:0.60098, acc: 0.73989, mcc: 0.52124, sn: 0.54447, sp: 0.93531, auc: 0.84174; test loss: 0.61466, acc: 0.74000, mcc: 0.50487, sn: 0.58500, sp: 0.89500, auc: 0.82915;\n",
      "\n",
      "epoch: 2; train loss:0.56235, acc: 0.76415, mcc: 0.54818, sn: 0.63073, sp: 0.89757, auc: 0.85447; test loss: 0.59682, acc: 0.73500, mcc: 0.48072, sn: 0.63000, sp: 0.84000, auc: 0.83332;\n",
      "\n",
      "epoch: 3; train loss:0.53498, acc: 0.77022, mcc: 0.55650, sn: 0.65094, sp: 0.88949, auc: 0.86037; test loss: 0.58962, acc: 0.73500, mcc: 0.48072, sn: 0.63000, sp: 0.84000, auc: 0.83375;\n",
      "\n",
      "epoch: 4; train loss:0.51538, acc: 0.77460, mcc: 0.56321, sn: 0.66375, sp: 0.88544, auc: 0.86053; test loss: 0.58575, acc: 0.75500, mcc: 0.51946, sn: 0.66000, sp: 0.85000, auc: 0.83518;\n",
      "\n",
      "epoch: 5; train loss:0.49899, acc: 0.77392, mcc: 0.55854, sn: 0.67655, sp: 0.87129, auc: 0.86525; test loss: 0.57969, acc: 0.75500, mcc: 0.51507, sn: 0.68500, sp: 0.82500, auc: 0.83612;\n",
      "\n",
      "epoch: 6; train loss:0.48261, acc: 0.77864, mcc: 0.56960, sn: 0.67520, sp: 0.88208, auc: 0.86833; test loss: 0.57834, acc: 0.76000, mcc: 0.52445, sn: 0.69500, sp: 0.82500, auc: 0.83680;\n",
      "\n",
      "epoch: 7; train loss:0.48225, acc: 0.77662, mcc: 0.56132, sn: 0.69205, sp: 0.86119, auc: 0.86638; test loss: 0.57446, acc: 0.77000, mcc: 0.54220, sn: 0.72500, sp: 0.81500, auc: 0.83725;\n",
      "\n",
      "epoch: 8; train loss:0.47375, acc: 0.78403, mcc: 0.57470, sn: 0.70822, sp: 0.85984, auc: 0.87046; test loss: 0.57964, acc: 0.76750, mcc: 0.53857, sn: 0.71000, sp: 0.82500, auc: 0.83662;\n",
      "\n",
      "epoch: 9; train loss:0.46694, acc: 0.78673, mcc: 0.58028, sn: 0.71024, sp: 0.86321, auc: 0.87243; test loss: 0.58369, acc: 0.76750, mcc: 0.53857, sn: 0.71000, sp: 0.82500, auc: 0.83657;\n",
      "\n",
      "epoch: 10; train loss:0.45855, acc: 0.78706, mcc: 0.58387, sn: 0.69609, sp: 0.87803, auc: 0.87140; test loss: 0.58052, acc: 0.76250, mcc: 0.52739, sn: 0.71500, sp: 0.81000, auc: 0.83642;\n",
      "\n",
      "epoch: 11; train loss:0.45169, acc: 0.78976, mcc: 0.58529, sn: 0.71968, sp: 0.85984, auc: 0.87401; test loss: 0.58442, acc: 0.75750, mcc: 0.51786, sn: 0.70500, sp: 0.81000, auc: 0.83653;\n",
      "\n",
      "epoch: 12; train loss:0.45477, acc: 0.79144, mcc: 0.58875, sn: 0.72102, sp: 0.86186, auc: 0.87249; test loss: 0.58520, acc: 0.75500, mcc: 0.51257, sn: 0.70500, sp: 0.80500, auc: 0.83647;\n",
      "\n",
      "epoch: 13; train loss:0.44906, acc: 0.78774, mcc: 0.58110, sn: 0.71833, sp: 0.85714, auc: 0.87440; test loss: 0.58679, acc: 0.75500, mcc: 0.51257, sn: 0.70500, sp: 0.80500, auc: 0.83595;\n",
      "\n",
      "epoch: 14; train loss:0.45006, acc: 0.78437, mcc: 0.57397, sn: 0.71698, sp: 0.85175, auc: 0.87382; test loss: 0.58947, acc: 0.75250, mcc: 0.50729, sn: 0.70500, sp: 0.80000, auc: 0.83603;\n",
      "\n",
      "epoch: 15; train loss:0.45015, acc: 0.78706, mcc: 0.57963, sn: 0.71833, sp: 0.85580, auc: 0.87501; test loss: 0.58797, acc: 0.76000, mcc: 0.52167, sn: 0.72000, sp: 0.80000, auc: 0.83588;\n",
      "\n",
      "epoch: 16; train loss:0.45186, acc: 0.78942, mcc: 0.58211, sn: 0.73652, sp: 0.84232, auc: 0.87533; test loss: 0.59355, acc: 0.75750, mcc: 0.51687, sn: 0.71500, sp: 0.80000, auc: 0.83550;\n",
      "\n",
      "epoch: 17; train loss:0.45238, acc: 0.79009, mcc: 0.58638, sn: 0.71765, sp: 0.86253, auc: 0.87548; test loss: 0.59660, acc: 0.75250, mcc: 0.50729, sn: 0.70500, sp: 0.80000, auc: 0.83557;\n",
      "\n",
      "epoch: 18; train loss:0.45058, acc: 0.78673, mcc: 0.57739, sn: 0.72844, sp: 0.84501, auc: 0.87434; test loss: 0.59675, acc: 0.75750, mcc: 0.51687, sn: 0.71500, sp: 0.80000, auc: 0.83560;\n",
      "\n",
      "epoch: 19; train loss:0.44710, acc: 0.78976, mcc: 0.58432, sn: 0.72574, sp: 0.85377, auc: 0.87466; test loss: 0.59835, acc: 0.75750, mcc: 0.51687, sn: 0.71500, sp: 0.80000, auc: 0.83540;\n",
      "\n",
      "epoch: 20; train loss:0.43932, acc: 0.78976, mcc: 0.58453, sn: 0.72439, sp: 0.85512, auc: 0.87694; test loss: 0.59727, acc: 0.75750, mcc: 0.51645, sn: 0.72000, sp: 0.79500, auc: 0.83527;\n",
      "\n",
      "epoch: 21; train loss:0.44027, acc: 0.78908, mcc: 0.58164, sn: 0.73450, sp: 0.84367, auc: 0.87708; test loss: 0.60104, acc: 0.75500, mcc: 0.51164, sn: 0.71500, sp: 0.79500, auc: 0.83517;\n",
      "\n",
      "epoch: 22; train loss:0.44836, acc: 0.78605, mcc: 0.57867, sn: 0.71092, sp: 0.86119, auc: 0.87489; test loss: 0.60125, acc: 0.75750, mcc: 0.51645, sn: 0.72000, sp: 0.79500, auc: 0.83510;\n",
      "\n",
      "epoch: 23; train loss:0.44594, acc: 0.79313, mcc: 0.58944, sn: 0.74124, sp: 0.84501, auc: 0.87781; test loss: 0.60159, acc: 0.75500, mcc: 0.51125, sn: 0.72000, sp: 0.79000, auc: 0.83520;\n",
      "\n",
      "epoch: 24; train loss:0.44213, acc: 0.79279, mcc: 0.58888, sn: 0.73989, sp: 0.84569, auc: 0.87647; test loss: 0.60209, acc: 0.75750, mcc: 0.51609, sn: 0.72500, sp: 0.79000, auc: 0.83535;\n",
      "\n",
      "epoch: 25; train loss:0.43883, acc: 0.79279, mcc: 0.59060, sn: 0.72776, sp: 0.85782, auc: 0.87830; test loss: 0.60653, acc: 0.74750, mcc: 0.49680, sn: 0.70500, sp: 0.79000, auc: 0.83508;\n",
      "\n",
      "epoch: 26; train loss:0.43329, acc: 0.79380, mcc: 0.59079, sn: 0.74191, sp: 0.84569, auc: 0.87864; test loss: 0.60663, acc: 0.75250, mcc: 0.50643, sn: 0.71500, sp: 0.79000, auc: 0.83520;\n",
      "\n",
      "epoch: 27; train loss:0.43665, acc: 0.79279, mcc: 0.59182, sn: 0.72035, sp: 0.86523, auc: 0.87708; test loss: 0.60869, acc: 0.74500, mcc: 0.49200, sn: 0.70000, sp: 0.79000, auc: 0.83525;\n",
      "\n",
      "epoch: 28; train loss:0.43776, acc: 0.79346, mcc: 0.59024, sn: 0.74057, sp: 0.84636, auc: 0.87832; test loss: 0.60906, acc: 0.75000, mcc: 0.50161, sn: 0.71000, sp: 0.79000, auc: 0.83522;\n",
      "\n",
      "epoch: 29; train loss:0.43638, acc: 0.79447, mcc: 0.59437, sn: 0.72709, sp: 0.86186, auc: 0.87892; test loss: 0.60701, acc: 0.75250, mcc: 0.50551, sn: 0.73000, sp: 0.77500, auc: 0.83515;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../4-new-12w-0 were not used when initializing C_Bert_average_embedding: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing C_Bert_average_embedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing C_Bert_average_embedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; train loss:0.66493, acc: 0.61725, mcc: 0.29647, sn: 0.31132, sp: 0.92318, auc: 0.71093; test loss: 0.64973, acc: 0.65750, mcc: 0.37570, sn: 0.38500, sp: 0.93000, auc: 0.81625;\n",
      "\n",
      "epoch: 1; train loss:0.60000, acc: 0.74090, mcc: 0.52168, sn: 0.54919, sp: 0.93261, auc: 0.84301; test loss: 0.62190, acc: 0.72500, mcc: 0.47498, sn: 0.56500, sp: 0.88500, auc: 0.82220;\n",
      "\n",
      "epoch: 2; train loss:0.55712, acc: 0.76853, mcc: 0.55652, sn: 0.63747, sp: 0.89960, auc: 0.85610; test loss: 0.61134, acc: 0.74000, mcc: 0.49710, sn: 0.61000, sp: 0.87000, auc: 0.82458;\n",
      "\n",
      "epoch: 3; train loss:0.52119, acc: 0.77392, mcc: 0.56384, sn: 0.65566, sp: 0.89218, auc: 0.86067; test loss: 0.60724, acc: 0.74000, mcc: 0.49445, sn: 0.62000, sp: 0.86000, auc: 0.82485;\n",
      "\n",
      "epoch: 4; train loss:0.50650, acc: 0.77864, mcc: 0.56993, sn: 0.67385, sp: 0.88342, auc: 0.86340; test loss: 0.60256, acc: 0.75500, mcc: 0.51946, sn: 0.66000, sp: 0.85000, auc: 0.82537;\n",
      "\n",
      "epoch: 5; train loss:0.48663, acc: 0.77796, mcc: 0.56617, sn: 0.68329, sp: 0.87264, auc: 0.86642; test loss: 0.60634, acc: 0.75000, mcc: 0.50928, sn: 0.65500, sp: 0.84500, auc: 0.82520;\n",
      "\n",
      "epoch: 6; train loss:0.47907, acc: 0.78167, mcc: 0.57138, sn: 0.69811, sp: 0.86523, auc: 0.86683; test loss: 0.60785, acc: 0.75000, mcc: 0.50739, sn: 0.66500, sp: 0.83500, auc: 0.82523;\n",
      "\n",
      "epoch: 7; train loss:0.46889, acc: 0.78369, mcc: 0.57483, sn: 0.70350, sp: 0.86388, auc: 0.86791; test loss: 0.61075, acc: 0.75250, mcc: 0.51118, sn: 0.67500, sp: 0.83000, auc: 0.82600;\n",
      "\n",
      "epoch: 8; train loss:0.46442, acc: 0.77965, mcc: 0.56741, sn: 0.69542, sp: 0.86388, auc: 0.86852; test loss: 0.61208, acc: 0.75000, mcc: 0.50572, sn: 0.67500, sp: 0.82500, auc: 0.82595;\n",
      "\n",
      "epoch: 9; train loss:0.46190, acc: 0.78336, mcc: 0.57287, sn: 0.71024, sp: 0.85647, auc: 0.86994; test loss: 0.61229, acc: 0.75000, mcc: 0.50364, sn: 0.69000, sp: 0.81000, auc: 0.82610;\n",
      "\n",
      "epoch: 10; train loss:0.45707, acc: 0.78470, mcc: 0.57350, sn: 0.72507, sp: 0.84434, auc: 0.86995; test loss: 0.61775, acc: 0.74750, mcc: 0.49957, sn: 0.68000, sp: 0.81500, auc: 0.82663;\n",
      "\n",
      "epoch: 11; train loss:0.45460, acc: 0.78133, mcc: 0.56769, sn: 0.71496, sp: 0.84771, auc: 0.87014; test loss: 0.62314, acc: 0.74750, mcc: 0.49957, sn: 0.68000, sp: 0.81500, auc: 0.82650;\n",
      "\n",
      "epoch: 12; train loss:0.45490, acc: 0.78774, mcc: 0.57885, sn: 0.73383, sp: 0.84164, auc: 0.87260; test loss: 0.62865, acc: 0.74750, mcc: 0.49957, sn: 0.68000, sp: 0.81500, auc: 0.82682;\n",
      "\n",
      "epoch: 13; train loss:0.44535, acc: 0.78706, mcc: 0.57869, sn: 0.72439, sp: 0.84973, auc: 0.87374; test loss: 0.62622, acc: 0.74750, mcc: 0.49891, sn: 0.68500, sp: 0.81000, auc: 0.82708;\n",
      "\n",
      "epoch: 14; train loss:0.44982, acc: 0.78336, mcc: 0.57177, sn: 0.71698, sp: 0.84973, auc: 0.87183; test loss: 0.63243, acc: 0.75000, mcc: 0.50428, sn: 0.68500, sp: 0.81500, auc: 0.82698;\n",
      "\n",
      "epoch: 15; train loss:0.45262, acc: 0.78336, mcc: 0.57146, sn: 0.71900, sp: 0.84771, auc: 0.87323; test loss: 0.63564, acc: 0.74750, mcc: 0.49957, sn: 0.68000, sp: 0.81500, auc: 0.82773;\n",
      "\n",
      "epoch: 16; train loss:0.45395, acc: 0.78369, mcc: 0.57420, sn: 0.70687, sp: 0.86051, auc: 0.87283; test loss: 0.63010, acc: 0.76000, mcc: 0.52262, sn: 0.71000, sp: 0.81000, auc: 0.82775;\n",
      "\n",
      "epoch: 17; train loss:0.44631, acc: 0.79043, mcc: 0.58271, sn: 0.75067, sp: 0.83019, auc: 0.87570; test loss: 0.63797, acc: 0.75500, mcc: 0.51371, sn: 0.69500, sp: 0.81500, auc: 0.82753;\n",
      "\n",
      "epoch: 18; train loss:0.44745, acc: 0.78706, mcc: 0.57839, sn: 0.72642, sp: 0.84771, auc: 0.87292; test loss: 0.63845, acc: 0.75250, mcc: 0.50899, sn: 0.69000, sp: 0.81500, auc: 0.82775;\n",
      "\n",
      "epoch: 19; train loss:0.44897, acc: 0.78807, mcc: 0.57983, sn: 0.73181, sp: 0.84434, auc: 0.87329; test loss: 0.64180, acc: 0.75250, mcc: 0.50899, sn: 0.69000, sp: 0.81500, auc: 0.82772;\n",
      "\n",
      "epoch: 20; train loss:0.44067, acc: 0.79077, mcc: 0.58601, sn: 0.72911, sp: 0.85243, auc: 0.87474; test loss: 0.64156, acc: 0.75250, mcc: 0.50899, sn: 0.69000, sp: 0.81500, auc: 0.82800;\n",
      "\n",
      "epoch: 21; train loss:0.44706, acc: 0.78504, mcc: 0.57302, sn: 0.73450, sp: 0.83558, auc: 0.87419; test loss: 0.63781, acc: 0.76250, mcc: 0.52792, sn: 0.71000, sp: 0.81500, auc: 0.82805;\n",
      "\n",
      "epoch: 22; train loss:0.44290, acc: 0.78976, mcc: 0.58317, sn: 0.73383, sp: 0.84569, auc: 0.87466; test loss: 0.64928, acc: 0.75000, mcc: 0.50497, sn: 0.68000, sp: 0.82000, auc: 0.82780;\n",
      "\n",
      "epoch: 23; train loss:0.44475, acc: 0.78336, mcc: 0.57167, sn: 0.71765, sp: 0.84906, auc: 0.87513; test loss: 0.63597, acc: 0.76000, mcc: 0.52167, sn: 0.72000, sp: 0.80000, auc: 0.82792;\n",
      "\n",
      "epoch: 24; train loss:0.44377, acc: 0.79043, mcc: 0.58290, sn: 0.74865, sp: 0.83221, auc: 0.87544; test loss: 0.64424, acc: 0.76000, mcc: 0.52378, sn: 0.70000, sp: 0.82000, auc: 0.82785;\n",
      "\n",
      "epoch: 25; train loss:0.43909, acc: 0.78673, mcc: 0.57857, sn: 0.72035, sp: 0.85310, auc: 0.87649; test loss: 0.64136, acc: 0.76000, mcc: 0.52262, sn: 0.71000, sp: 0.81000, auc: 0.82790;\n",
      "\n",
      "epoch: 26; train loss:0.44813, acc: 0.79009, mcc: 0.58390, sn: 0.73383, sp: 0.84636, auc: 0.87437; test loss: 0.64333, acc: 0.76000, mcc: 0.52317, sn: 0.70500, sp: 0.81500, auc: 0.82820;\n",
      "\n",
      "epoch: 27; train loss:0.43991, acc: 0.78942, mcc: 0.58272, sn: 0.73181, sp: 0.84704, auc: 0.87530; test loss: 0.64282, acc: 0.76250, mcc: 0.52739, sn: 0.71500, sp: 0.81000, auc: 0.82815;\n",
      "\n",
      "epoch: 28; train loss:0.43819, acc: 0.78639, mcc: 0.57694, sn: 0.72642, sp: 0.84636, auc: 0.87653; test loss: 0.65428, acc: 0.75750, mcc: 0.51907, sn: 0.69500, sp: 0.82000, auc: 0.82807;\n",
      "\n",
      "epoch: 29; train loss:0.44308, acc: 0.79313, mcc: 0.59061, sn: 0.73248, sp: 0.85377, auc: 0.87689; test loss: 0.64541, acc: 0.76250, mcc: 0.52739, sn: 0.71500, sp: 0.81000, auc: 0.82842;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../5-new-12w-0 were not used when initializing C_Bert_average_embedding: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing C_Bert_average_embedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing C_Bert_average_embedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; train loss:0.65486, acc: 0.64623, mcc: 0.35885, sn: 0.35647, sp: 0.93598, auc: 0.75754; test loss: 0.64541, acc: 0.67750, mcc: 0.40594, sn: 0.43500, sp: 0.92000, auc: 0.81378;\n",
      "\n",
      "epoch: 1; train loss:0.60069, acc: 0.72069, mcc: 0.48903, sn: 0.50539, sp: 0.93598, auc: 0.83404; test loss: 0.61750, acc: 0.72250, mcc: 0.46021, sn: 0.59500, sp: 0.85000, auc: 0.81765;\n",
      "\n",
      "epoch: 2; train loss:0.55686, acc: 0.76044, mcc: 0.53956, sn: 0.63005, sp: 0.89084, auc: 0.85030; test loss: 0.61684, acc: 0.71000, mcc: 0.44176, sn: 0.55500, sp: 0.86500, auc: 0.81867;\n",
      "\n",
      "epoch: 3; train loss:0.52907, acc: 0.76482, mcc: 0.54692, sn: 0.64016, sp: 0.88949, auc: 0.85691; test loss: 0.60325, acc: 0.73000, mcc: 0.47267, sn: 0.61500, sp: 0.84500, auc: 0.81988;\n",
      "\n",
      "epoch: 4; train loss:0.50744, acc: 0.76819, mcc: 0.54865, sn: 0.66307, sp: 0.87332, auc: 0.85818; test loss: 0.60385, acc: 0.73500, mcc: 0.48180, sn: 0.62500, sp: 0.84500, auc: 0.82010;\n",
      "\n",
      "epoch: 5; train loss:0.49361, acc: 0.77358, mcc: 0.56070, sn: 0.66442, sp: 0.88275, auc: 0.86076; test loss: 0.59857, acc: 0.73750, mcc: 0.48244, sn: 0.65000, sp: 0.82500, auc: 0.82000;\n",
      "\n",
      "epoch: 6; train loss:0.49042, acc: 0.77325, mcc: 0.55598, sn: 0.68127, sp: 0.86523, auc: 0.86101; test loss: 0.60176, acc: 0.74250, mcc: 0.49260, sn: 0.65500, sp: 0.83000, auc: 0.82045;\n",
      "\n",
      "epoch: 7; train loss:0.47753, acc: 0.77594, mcc: 0.55957, sn: 0.69340, sp: 0.85849, auc: 0.86278; test loss: 0.60193, acc: 0.73750, mcc: 0.48160, sn: 0.65500, sp: 0.82000, auc: 0.82022;\n",
      "\n",
      "epoch: 8; train loss:0.46777, acc: 0.77830, mcc: 0.56678, sn: 0.68396, sp: 0.87264, auc: 0.86742; test loss: 0.60393, acc: 0.74250, mcc: 0.49093, sn: 0.66500, sp: 0.82000, auc: 0.82020;\n",
      "\n",
      "epoch: 9; train loss:0.47129, acc: 0.77628, mcc: 0.56057, sn: 0.69205, sp: 0.86051, auc: 0.86471; test loss: 0.61100, acc: 0.74500, mcc: 0.49814, sn: 0.65500, sp: 0.83500, auc: 0.82050;\n",
      "\n",
      "epoch: 10; train loss:0.46358, acc: 0.77594, mcc: 0.55995, sn: 0.69137, sp: 0.86051, auc: 0.86619; test loss: 0.60671, acc: 0.74750, mcc: 0.49831, sn: 0.69000, sp: 0.80500, auc: 0.82025;\n",
      "\n",
      "epoch: 11; train loss:0.46059, acc: 0.78167, mcc: 0.56918, sn: 0.71024, sp: 0.85310, auc: 0.86861; test loss: 0.61304, acc: 0.74500, mcc: 0.49561, sn: 0.67000, sp: 0.82000, auc: 0.82037;\n",
      "\n",
      "epoch: 12; train loss:0.46022, acc: 0.77965, mcc: 0.56555, sn: 0.70553, sp: 0.85377, auc: 0.86839; test loss: 0.61065, acc: 0.74250, mcc: 0.48770, sn: 0.69000, sp: 0.79500, auc: 0.82012;\n",
      "\n",
      "epoch: 13; train loss:0.46248, acc: 0.77898, mcc: 0.56098, sn: 0.72709, sp: 0.83086, auc: 0.86739; test loss: 0.62007, acc: 0.74750, mcc: 0.50106, sn: 0.67000, sp: 0.82500, auc: 0.82065;\n",
      "\n",
      "epoch: 14; train loss:0.46270, acc: 0.77493, mcc: 0.55695, sn: 0.69542, sp: 0.85445, auc: 0.86634; test loss: 0.61609, acc: 0.74750, mcc: 0.49891, sn: 0.68500, sp: 0.81000, auc: 0.82063;\n",
      "\n",
      "epoch: 15; train loss:0.45578, acc: 0.78167, mcc: 0.56802, sn: 0.71765, sp: 0.84569, auc: 0.86976; test loss: 0.61715, acc: 0.74500, mcc: 0.49357, sn: 0.68500, sp: 0.80500, auc: 0.82092;\n",
      "\n",
      "epoch: 16; train loss:0.45514, acc: 0.77527, mcc: 0.55710, sn: 0.69879, sp: 0.85175, auc: 0.86756; test loss: 0.61328, acc: 0.74500, mcc: 0.49158, sn: 0.70500, sp: 0.78500, auc: 0.82065;\n",
      "\n",
      "epoch: 17; train loss:0.45662, acc: 0.78100, mcc: 0.56554, sn: 0.72507, sp: 0.83693, auc: 0.86827; test loss: 0.62033, acc: 0.74500, mcc: 0.49419, sn: 0.68000, sp: 0.81000, auc: 0.82090;\n",
      "\n",
      "epoch: 18; train loss:0.44886, acc: 0.78133, mcc: 0.56729, sn: 0.71765, sp: 0.84501, auc: 0.87054; test loss: 0.61624, acc: 0.74750, mcc: 0.49680, sn: 0.70500, sp: 0.79000, auc: 0.82070;\n",
      "\n",
      "epoch: 19; train loss:0.45863, acc: 0.77796, mcc: 0.56079, sn: 0.71226, sp: 0.84367, auc: 0.86886; test loss: 0.61923, acc: 0.75000, mcc: 0.50204, sn: 0.70500, sp: 0.79500, auc: 0.82080;\n",
      "\n",
      "epoch: 20; train loss:0.44706, acc: 0.78538, mcc: 0.57650, sn: 0.71496, sp: 0.85580, auc: 0.87193; test loss: 0.61663, acc: 0.75250, mcc: 0.50643, sn: 0.71500, sp: 0.79000, auc: 0.82105;\n",
      "\n",
      "epoch: 21; train loss:0.44635, acc: 0.78268, mcc: 0.56916, sn: 0.72507, sp: 0.84030, auc: 0.87249; test loss: 0.62087, acc: 0.74500, mcc: 0.49247, sn: 0.69500, sp: 0.79500, auc: 0.82095;\n",
      "\n",
      "epoch: 22; train loss:0.44777, acc: 0.78032, mcc: 0.56402, sn: 0.72574, sp: 0.83491, auc: 0.87362; test loss: 0.62650, acc: 0.74000, mcc: 0.48349, sn: 0.68000, sp: 0.80000, auc: 0.82095;\n",
      "\n",
      "epoch: 23; train loss:0.44314, acc: 0.78571, mcc: 0.57540, sn: 0.72709, sp: 0.84434, auc: 0.87298; test loss: 0.62969, acc: 0.74500, mcc: 0.49419, sn: 0.68000, sp: 0.81000, auc: 0.82105;\n",
      "\n",
      "epoch: 24; train loss:0.44766, acc: 0.78268, mcc: 0.57073, sn: 0.71429, sp: 0.85108, auc: 0.87374; test loss: 0.62131, acc: 0.75000, mcc: 0.50161, sn: 0.71000, sp: 0.79000, auc: 0.82125;\n",
      "\n",
      "epoch: 25; train loss:0.44166, acc: 0.79111, mcc: 0.58580, sn: 0.73585, sp: 0.84636, auc: 0.87335; test loss: 0.62182, acc: 0.75000, mcc: 0.50161, sn: 0.71000, sp: 0.79000, auc: 0.82107;\n",
      "\n",
      "epoch: 26; train loss:0.44068, acc: 0.78268, mcc: 0.56943, sn: 0.72305, sp: 0.84232, auc: 0.87528; test loss: 0.62878, acc: 0.74250, mcc: 0.48770, sn: 0.69000, sp: 0.79500, auc: 0.82142;\n",
      "\n",
      "epoch: 27; train loss:0.44675, acc: 0.79043, mcc: 0.58509, sn: 0.73046, sp: 0.85040, auc: 0.87250; test loss: 0.63279, acc: 0.74750, mcc: 0.49957, sn: 0.68000, sp: 0.81500, auc: 0.82150;\n",
      "\n",
      "epoch: 28; train loss:0.44991, acc: 0.78673, mcc: 0.57757, sn: 0.72709, sp: 0.84636, auc: 0.87373; test loss: 0.62016, acc: 0.74500, mcc: 0.49120, sn: 0.71000, sp: 0.78000, auc: 0.82137;\n",
      "\n",
      "epoch: 29; train loss:0.43396, acc: 0.78875, mcc: 0.58067, sn: 0.73652, sp: 0.84097, auc: 0.87737; test loss: 0.63317, acc: 0.74750, mcc: 0.49957, sn: 0.68000, sp: 0.81500, auc: 0.82127;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../6-new-12w-0 were not used when initializing C_Bert_average_embedding: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing C_Bert_average_embedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing C_Bert_average_embedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; train loss:0.66309, acc: 0.62197, mcc: 0.32037, sn: 0.29784, sp: 0.94609, auc: 0.74477; test loss: 0.65485, acc: 0.66000, mcc: 0.38316, sn: 0.38500, sp: 0.93500, auc: 0.81255;\n",
      "\n",
      "epoch: 1; train loss:0.62076, acc: 0.71260, mcc: 0.47491, sn: 0.48989, sp: 0.93531, auc: 0.83545; test loss: 0.63170, acc: 0.71000, mcc: 0.44836, sn: 0.53500, sp: 0.88500, auc: 0.81945;\n",
      "\n",
      "epoch: 2; train loss:0.58704, acc: 0.74191, mcc: 0.51207, sn: 0.57817, sp: 0.90566, auc: 0.84257; test loss: 0.61742, acc: 0.72500, mcc: 0.46355, sn: 0.60500, sp: 0.84500, auc: 0.82000;\n",
      "\n",
      "epoch: 3; train loss:0.56027, acc: 0.76213, mcc: 0.53609, sn: 0.65768, sp: 0.86658, auc: 0.84889; test loss: 0.61011, acc: 0.73000, mcc: 0.47267, sn: 0.61500, sp: 0.84500, auc: 0.82102;\n",
      "\n",
      "epoch: 4; train loss:0.53993, acc: 0.75371, mcc: 0.52982, sn: 0.60984, sp: 0.89757, auc: 0.84901; test loss: 0.59760, acc: 0.74500, mcc: 0.49724, sn: 0.66000, sp: 0.83000, auc: 0.82075;\n",
      "\n",
      "epoch: 5; train loss:0.51921, acc: 0.76853, mcc: 0.54429, sn: 0.68733, sp: 0.84973, auc: 0.85344; test loss: 0.59865, acc: 0.74250, mcc: 0.49352, sn: 0.65000, sp: 0.83500, auc: 0.82147;\n",
      "\n",
      "epoch: 6; train loss:0.51194, acc: 0.76718, mcc: 0.54732, sn: 0.65903, sp: 0.87534, auc: 0.85469; test loss: 0.59660, acc: 0.75000, mcc: 0.50739, sn: 0.66500, sp: 0.83500, auc: 0.82182;\n",
      "\n",
      "epoch: 7; train loss:0.50237, acc: 0.76651, mcc: 0.54357, sn: 0.66846, sp: 0.86456, auc: 0.85768; test loss: 0.59678, acc: 0.75000, mcc: 0.50653, sn: 0.67000, sp: 0.83000, auc: 0.82265;\n",
      "\n",
      "epoch: 8; train loss:0.49357, acc: 0.77089, mcc: 0.54817, sn: 0.69474, sp: 0.84704, auc: 0.85808; test loss: 0.59743, acc: 0.75000, mcc: 0.50572, sn: 0.67500, sp: 0.82500, auc: 0.82330;\n",
      "\n",
      "epoch: 9; train loss:0.48803, acc: 0.77325, mcc: 0.55300, sn: 0.69677, sp: 0.84973, auc: 0.85900; test loss: 0.59903, acc: 0.75250, mcc: 0.51039, sn: 0.68000, sp: 0.82500, auc: 0.82393;\n",
      "\n",
      "epoch: 10; train loss:0.48275, acc: 0.77022, mcc: 0.54521, sn: 0.70418, sp: 0.83625, auc: 0.85780; test loss: 0.60133, acc: 0.75500, mcc: 0.51584, sn: 0.68000, sp: 0.83000, auc: 0.82403;\n",
      "\n",
      "epoch: 11; train loss:0.47720, acc: 0.77190, mcc: 0.55075, sn: 0.69272, sp: 0.85108, auc: 0.85979; test loss: 0.60022, acc: 0.76500, mcc: 0.53386, sn: 0.70500, sp: 0.82500, auc: 0.82412;\n",
      "\n",
      "epoch: 12; train loss:0.47526, acc: 0.77561, mcc: 0.55726, sn: 0.70216, sp: 0.84906, auc: 0.86191; test loss: 0.60473, acc: 0.75750, mcc: 0.52050, sn: 0.68500, sp: 0.83000, auc: 0.82377;\n",
      "\n",
      "epoch: 13; train loss:0.47218, acc: 0.77527, mcc: 0.55525, sn: 0.71024, sp: 0.84030, auc: 0.86398; test loss: 0.60140, acc: 0.76250, mcc: 0.52792, sn: 0.71000, sp: 0.81500, auc: 0.82390;\n",
      "\n",
      "epoch: 14; train loss:0.46803, acc: 0.78167, mcc: 0.56735, sn: 0.72237, sp: 0.84097, auc: 0.86387; test loss: 0.60778, acc: 0.75750, mcc: 0.52050, sn: 0.68500, sp: 0.83000, auc: 0.82420;\n",
      "\n",
      "epoch: 15; train loss:0.46273, acc: 0.77999, mcc: 0.56497, sn: 0.71361, sp: 0.84636, auc: 0.86571; test loss: 0.60854, acc: 0.75750, mcc: 0.51976, sn: 0.69000, sp: 0.82500, auc: 0.82475;\n",
      "\n",
      "epoch: 16; train loss:0.45638, acc: 0.78032, mcc: 0.56657, sn: 0.70822, sp: 0.85243, auc: 0.86759; test loss: 0.60426, acc: 0.76500, mcc: 0.53130, sn: 0.73000, sp: 0.80000, auc: 0.82470;\n",
      "\n",
      "epoch: 17; train loss:0.45985, acc: 0.77628, mcc: 0.55786, sn: 0.70755, sp: 0.84501, auc: 0.86483; test loss: 0.60903, acc: 0.76250, mcc: 0.52739, sn: 0.71500, sp: 0.81000, auc: 0.82482;\n",
      "\n",
      "epoch: 18; train loss:0.45927, acc: 0.78032, mcc: 0.56560, sn: 0.71429, sp: 0.84636, auc: 0.86559; test loss: 0.61025, acc: 0.76750, mcc: 0.53743, sn: 0.72000, sp: 0.81500, auc: 0.82480;\n",
      "\n",
      "epoch: 19; train loss:0.46036, acc: 0.78302, mcc: 0.56979, sn: 0.72574, sp: 0.84030, auc: 0.86539; test loss: 0.60606, acc: 0.76000, mcc: 0.52128, sn: 0.72500, sp: 0.79500, auc: 0.82503;\n",
      "\n",
      "epoch: 20; train loss:0.46232, acc: 0.78336, mcc: 0.56975, sn: 0.73181, sp: 0.83491, auc: 0.86715; test loss: 0.60977, acc: 0.76750, mcc: 0.53694, sn: 0.72500, sp: 0.81000, auc: 0.82527;\n",
      "\n",
      "epoch: 21; train loss:0.45903, acc: 0.78673, mcc: 0.57686, sn: 0.73248, sp: 0.84097, auc: 0.86911; test loss: 0.61436, acc: 0.76750, mcc: 0.53743, sn: 0.72000, sp: 0.81500, auc: 0.82553;\n",
      "\n",
      "epoch: 22; train loss:0.45323, acc: 0.78167, mcc: 0.56565, sn: 0.73652, sp: 0.82682, auc: 0.86904; test loss: 0.61672, acc: 0.77000, mcc: 0.54330, sn: 0.71500, sp: 0.82500, auc: 0.82572;\n",
      "\n",
      "epoch: 23; train loss:0.45178, acc: 0.78133, mcc: 0.56739, sn: 0.71698, sp: 0.84569, auc: 0.86839; test loss: 0.61739, acc: 0.77250, mcc: 0.54803, sn: 0.72000, sp: 0.82500, auc: 0.82598;\n",
      "\n",
      "epoch: 24; train loss:0.45478, acc: 0.78470, mcc: 0.57481, sn: 0.71631, sp: 0.85310, auc: 0.87028; test loss: 0.61416, acc: 0.77250, mcc: 0.54654, sn: 0.73500, sp: 0.81000, auc: 0.82598;\n",
      "\n",
      "epoch: 25; train loss:0.45974, acc: 0.78201, mcc: 0.56665, sn: 0.73383, sp: 0.83019, auc: 0.86861; test loss: 0.61785, acc: 0.77250, mcc: 0.54748, sn: 0.72500, sp: 0.82000, auc: 0.82608;\n",
      "\n",
      "epoch: 26; train loss:0.44855, acc: 0.78841, mcc: 0.57963, sn: 0.73922, sp: 0.83760, auc: 0.87209; test loss: 0.62232, acc: 0.76250, mcc: 0.52915, sn: 0.70000, sp: 0.82500, auc: 0.82632;\n",
      "\n",
      "epoch: 27; train loss:0.44757, acc: 0.78403, mcc: 0.57205, sn: 0.72507, sp: 0.84299, auc: 0.87264; test loss: 0.61229, acc: 0.77000, mcc: 0.54133, sn: 0.73500, sp: 0.80500, auc: 0.82635;\n",
      "\n",
      "epoch: 28; train loss:0.44595, acc: 0.78807, mcc: 0.58001, sn: 0.73046, sp: 0.84569, auc: 0.87143; test loss: 0.61870, acc: 0.76750, mcc: 0.53797, sn: 0.71500, sp: 0.82000, auc: 0.82625;\n",
      "\n",
      "epoch: 29; train loss:0.45201, acc: 0.78706, mcc: 0.57839, sn: 0.72642, sp: 0.84771, auc: 0.87417; test loss: 0.61429, acc: 0.77500, mcc: 0.55135, sn: 0.74000, sp: 0.81000, auc: 0.82625;\n",
      "\n",
      "\n",
      "(400,)\n",
      "0.7675 0.5392293148264975 0.705 0.83 0.8306250000000001\n"
     ]
    }
   ],
   "source": [
    "# average [3,4,5,6]\n",
    "# 添加L2-正则化 average\n",
    "mers = [3, 4, 5, 6]\n",
    "test_real_labels_list, test_pre_labels_list = [], []\n",
    "\n",
    "filename = \"./result/record.txt\"\n",
    "file_log = open(filename, \"w\")\n",
    "content = \"state\\ttrain-loss\\tacc\\tmcc\\tsn\\tsp\\tauc\\tind-loss\\tacc\\tmcc\\tsn\\tsp\\tauc\\n\"\n",
    "file_log.write(content)\n",
    "file_log.flush()\n",
    "\n",
    "\n",
    "for i in range(len(mers)):\n",
    "    mer = mers[i]\n",
    "    \n",
    "    args.model_path = \"../../DNA-BERT-{}-new-12w-0\".format(mer)\n",
    "    args.ind_filename = \"../dataset/enhancer_{}-mer_DNABERT_ind.txt\".format(mer)\n",
    "    args.tra_filename = \"../dataset/enhancer_{}-mer_DNABERT_tra.txt\".format(mer)\n",
    "    \n",
    "    tra_emb_dataloader = getData(args, split=False, validation=False, shuffle=True)\n",
    "    ind_emb_dataloader = getData(args, split=False, validation=True, shuffle=False)\n",
    "    \n",
    "    emb_model = C_Bert_average_embedding.from_pretrained(args.model_path).to(device)\n",
    "\n",
    "    tra_emb_features, tra_labels = embedding(emb_model, tra_emb_dataloader, mer)\n",
    "    ind_emb_features, ind_labels = embedding(emb_model, ind_emb_dataloader, mer)\n",
    "\n",
    "    train_dataset = NewDataset_classifier(tra_emb_features, tra_labels)\n",
    "    tra_dataloader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle = True)\n",
    "\n",
    "    ind_dataset = NewDataset_classifier(ind_emb_features, ind_labels)\n",
    "    ind_dataloader = DataLoader(ind_dataset, batch_size = args.batch_size, shuffle = False)\n",
    "\n",
    "\n",
    "    classifier_model = Enhancer_classifier().to(device)\n",
    "    epoches = args.epoches\n",
    "    learning_rate = args.learning_rate\n",
    "    optimizer = optim.Adam(classifier_model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08,)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.98)    # 指数衰减损失函数\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        tra_loss, tra_acc, tra_mcc, tra_sn, tra_sp, tra_auc = train_classifier(classifier_model, tra_dataloader, optimizer)\n",
    "        ind_loss, ind_acc, ind_mcc, ind_sn, ind_sp, ind_auc, real_labels, pre_labels = test_classifier(classifier_model, ind_dataloader, optimizer)\n",
    "\n",
    "        content = \"epoch: {};\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f};\\n\".format(epoch, \\\n",
    "            tra_loss, tra_acc, tra_mcc, tra_sn, tra_sp, tra_auc, ind_loss, ind_acc, ind_mcc, ind_sn, ind_sp, ind_auc)\n",
    "        print(content)\n",
    "        # print(real_labels[0:20])\n",
    "\n",
    "        if epoch == 29:\n",
    "            test_real_labels_list.append(real_labels)\n",
    "            test_pre_labels_list.append(pre_labels)\n",
    "            file_log.write(content)\n",
    "            file_log.flush()\n",
    "\n",
    "print()\n",
    "test_real_labels = np.mean(test_real_labels_list, axis=0)\n",
    "test_pre_labels = np.mean(test_pre_labels_list, axis=0)\n",
    "\n",
    "print(test_real_labels.shape)\n",
    "\n",
    "acc, mcc, sn, sp = evaluation_criterion(test_pre_labels, test_real_labels)\n",
    "fpr, tpr, threshold = metrics.roc_curve(test_real_labels, test_pre_labels)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print(acc, mcc, sn, sp, roc_auc)\n",
    "content = \"integrate; acc: {:.5f}, mcc: {:.5f}, sn: {:.5f}, sp: {:.5f}, auc: {:.5f};\".format(acc, mcc, sn, sp, roc_auc)\n",
    "file_log.write(content)\n",
    "file_log.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_pytorch_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06233b56f961380e94bcca211189b77c55ac475925b9cac0cc39dad8759f75d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
