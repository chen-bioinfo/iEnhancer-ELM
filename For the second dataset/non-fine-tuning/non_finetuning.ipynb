{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiahao/anaconda3/envs/tensorflow_pytorch_python/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import * \n",
    "from model import *\n",
    "from pandas import DataFrame\n",
    "from sklearn import metrics\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epoches',              type=int,  default=30,  help='')\n",
    "parser.add_argument('--batch_size',           type=int,  default=128,  help='')\n",
    "parser.add_argument('--max_length',           type=int,  default=200, help='')\n",
    "parser.add_argument('--learning_rate',        type=float, default=1e-4, help=\"\")\n",
    "parser.add_argument('--model_path',           type=str,  default=\"../3-new-12w-0\", help='')\n",
    "parser.add_argument('--ind_filename',  type=str,  default=\"../dataset/enhancer_3-mer_DNABERT_ind.txt\", help='')\n",
    "parser.add_argument('--tra_filename',  type=str,  default=\"../dataset/enhancer_3-mer_DNABERT_tra.txt\", help='')\n",
    "\n",
    "args = parser.parse_args(args=[]) # 如果不使用\"args=[]\"，会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(sequences, DNABert, tokenizer):\n",
    "    \n",
    "    output_embeddings = []\n",
    "\n",
    "    for sequence in sequences:\n",
    "        sequence_1 = \" \".join(sequence.split(\" \")[0:500])\n",
    "        sequence_2 = \" \".join(sequence.split(\" \")[500:1000])\n",
    "        sequence_3 = \" \".join(sequence.split(\" \")[1000:1500])\n",
    "        sequence_4 = \" \".join(sequence.split(\" \")[1500:2000])\n",
    "\n",
    "        number_1 = len(sequence_1.split(\" \"))\n",
    "        number_2 = len(sequence_2.split(\" \"))\n",
    "        number_3 = len(sequence_3.split(\" \"))\n",
    "        number_4 = len(sequence_4.split(\" \"))\n",
    "\n",
    "        # sequence_1 = tokenizer(sequence_1, padding='max_length', truncation=True, max_length = 502)\n",
    "        # sequence_2 = tokenizer(sequence_2, padding='max_length', truncation=True, max_length = 502)\n",
    "        # sequence_3 = tokenizer(sequence_3, padding='max_length', truncation=True, max_length = 502)\n",
    "        # sequence_4 = tokenizer(sequence_4, padding='max_length', truncation=True, max_length = 502)\n",
    "\n",
    "        tokening = tokenizer([sequence_1, sequence_2, sequence_3, sequence_4], padding='max_length', truncation=True, max_length = 502)\n",
    " \n",
    "        batch_ids = torch.IntTensor(tokening[\"input_ids\"]).to(device)\n",
    "        batch_mask = torch.IntTensor(tokening[\"attention_mask\"]).to(device)\n",
    "        batch_number = torch.IntTensor([number_1, number_2, number_3, number_4]).to(device)\n",
    "\n",
    "        DNABert.eval()\n",
    "        with torch.no_grad():\n",
    "            embedding_1 = DNABert(batch_ids, batch_mask, batch_number).to(\"cpu\").numpy()\n",
    "        output_embeddings.append(embedding_1)\n",
    "\n",
    "    return output_embeddings\n",
    "\n",
    "\n",
    "def train_classifier(model, dataloader, optimizer):\n",
    "    train_iter, train_loss_sum = 0.0, 0.0\n",
    "    real_labels, pre_labels = [], []\n",
    "    for batch_data in dataloader:\n",
    "        batch_features = batch_data[\"features\"].to(device)\n",
    "        batch_labels = batch_data[\"labels\"].to(device)\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features, batch_labels)\n",
    "        \n",
    "        # 反向梯度信息\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute performance\n",
    "        train_loss = outputs[0].detach().to(\"cpu\").numpy()\n",
    "        logits = outputs[1].detach()\n",
    "\n",
    "        train_iter += 1\n",
    "        train_loss_sum += train_loss\n",
    "\n",
    "        if len(real_labels) == 0:\n",
    "            real_labels = batch_labels.to(\"cpu\").numpy()\n",
    "            pre_labels = logits.to(\"cpu\").numpy()\n",
    "        else:\n",
    "            real_labels = np.concatenate([real_labels, batch_labels.to(\"cpu\").numpy()], axis=0)\n",
    "            pre_labels = np.concatenate([pre_labels, logits.to(\"cpu\").numpy()], axis=0)\n",
    "\n",
    "    tra_loss = train_loss_sum/(train_iter)\n",
    "    acc, mcc, sn, sp = evaluation_criterion(pre_labels, real_labels)\n",
    "\n",
    "    fpr, tpr, threshold = metrics.roc_curve(real_labels, pre_labels)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return tra_loss, acc, mcc, sn, sp, roc_auc\n",
    "\n",
    "\n",
    "def test_classifier(model, dataloader, optimizer):\n",
    "    test_iter, test_loss_sum = 0.0, 0.0\n",
    "    real_labels, pre_labels = [], []\n",
    "    for batch_data in dataloader:\n",
    "        batch_features = batch_data[\"features\"].to(device)\n",
    "        batch_labels = batch_data[\"labels\"].to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_features, batch_labels)\n",
    "        \n",
    "        # compute performance\n",
    "        test_loss = outputs[0].detach().to(\"cpu\").numpy()\n",
    "        logits = outputs[1].detach()\n",
    "\n",
    "        test_iter += 1\n",
    "        test_loss_sum += test_loss\n",
    "\n",
    "        if len(real_labels) == 0:\n",
    "            real_labels = batch_labels.to(\"cpu\").numpy()\n",
    "            pre_labels = logits.to(\"cpu\").numpy()\n",
    "        else:\n",
    "            real_labels = np.concatenate([real_labels, batch_labels.to(\"cpu\").numpy()], axis=0)\n",
    "            pre_labels = np.concatenate([pre_labels, logits.to(\"cpu\").numpy()], axis=0)\n",
    "\n",
    "    test_loss = test_loss_sum/(test_iter)\n",
    "    acc, mcc, sn, sp = evaluation_criterion(pre_labels, real_labels)\n",
    "\n",
    "    fpr, tpr, threshold = metrics.roc_curve(real_labels, pre_labels)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return test_loss, acc, mcc, sn, sp, roc_auc, real_labels, pre_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../3-new-12w-0 were not used when initializing C_Bert_average_embedding: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing C_Bert_average_embedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing C_Bert_average_embedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Embedding end!!!--------------\n",
      "-----------------classifier-------------\n",
      "epoch=0-3mer-\t0.67491\t0.55698\t0.12614\t0.34267\t0.77130\t0.60245\t0.64412\t0.56379\t0.19233\t0.18966\t0.93793\t0.71300\t0.60727\t0.69361\t0.21372\t0.19436\t0.94323\t0.73337\n",
      "epoch=1-3mer-\t0.62708\t0.64256\t0.29718\t0.50153\t0.78358\t0.71962\t0.61150\t0.65690\t0.32307\t0.53793\t0.77586\t0.74583\t0.59442\t0.71520\t0.35982\t0.57473\t0.78544\t0.76326\n",
      "epoch=2-3mer-\t0.60618\t0.67287\t0.34911\t0.60361\t0.74213\t0.73996\t0.60260\t0.70690\t0.41864\t0.78276\t0.63103\t0.75819\t0.62463\t0.67672\t0.38727\t0.79089\t0.61963\t0.77462\n",
      "epoch=3-3mer-\t0.59278\t0.67479\t0.35134\t0.62471\t0.72487\t0.75091\t0.58350\t0.70172\t0.40577\t0.64828\t0.75517\t0.76295\t0.56756\t0.71969\t0.39433\t0.65427\t0.75241\t0.78108\n",
      "epoch=4-3mer-\t0.58497\t0.68611\t0.37442\t0.63200\t0.74021\t0.75749\t0.59615\t0.70690\t0.42717\t0.83103\t0.58276\t0.76661\t0.63108\t0.66945\t0.40478\t0.84413\t0.58210\t0.78519\n",
      "epoch=5-3mer-\t0.59044\t0.67114\t0.34300\t0.63891\t0.70338\t0.74690\t0.57665\t0.71379\t0.42824\t0.74138\t0.68621\t0.76723\t0.58070\t0.69852\t0.39711\t0.74471\t0.67543\t0.78714\n",
      "epoch=6-3mer-\t0.57770\t0.68438\t0.36984\t0.64620\t0.72256\t0.76245\t0.58327\t0.71552\t0.43147\t0.69310\t0.73793\t0.76971\t0.55815\t0.71926\t0.40991\t0.69852\t0.72963\t0.78977\n",
      "epoch=7-3mer-\t0.57149\t0.69589\t0.39287\t0.65886\t0.73292\t0.76805\t0.57599\t0.71552\t0.43217\t0.75172\t0.67931\t0.77094\t0.57927\t0.70173\t0.41112\t0.76588\t0.66966\t0.79112\n",
      "epoch=8-3mer-\t0.57011\t0.70184\t0.40422\t0.67613\t0.72755\t0.77180\t0.57069\t0.70690\t0.41428\t0.73103\t0.68276\t0.76911\t0.57209\t0.70558\t0.41087\t0.75241\t0.68217\t0.79116\n",
      "epoch=9-3mer-\t0.56671\t0.69743\t0.39556\t0.66769\t0.72717\t0.77296\t0.57617\t0.71379\t0.42882\t0.75172\t0.67586\t0.77251\t0.57333\t0.70366\t0.41251\t0.76267\t0.67415\t0.79394\n",
      "epoch=10-3mer-\t0.56437\t0.69877\t0.39814\t0.67153\t0.72602\t0.77521\t0.57568\t0.70862\t0.41997\t0.76552\t0.65172\t0.77378\t0.57963\t0.70237\t0.41839\t0.77999\t0.66357\t0.79438\n",
      "epoch=11-3mer-\t0.56157\t0.70530\t0.41078\t0.68995\t0.72064\t0.77848\t0.57582\t0.70345\t0.40768\t0.67241\t0.73448\t0.77303\t0.54657\t0.72012\t0.41102\t0.69788\t0.73124\t0.79492\n",
      "epoch=12-3mer-\t0.56488\t0.69398\t0.38849\t0.66769\t0.72026\t0.77459\t0.56078\t0.70172\t0.40364\t0.68621\t0.71724\t0.77551\t0.54746\t0.72055\t0.41341\t0.70237\t0.72963\t0.79595\n",
      "epoch=13-3mer-\t0.56547\t0.69493\t0.39010\t0.67767\t0.71220\t0.77331\t0.57522\t0.69138\t0.38932\t0.60000\t0.78276\t0.77414\t0.53013\t0.72889\t0.40405\t0.63759\t0.77453\t0.79579\n",
      "epoch=14-3mer-\t0.55970\t0.70721\t0.41477\t0.68688\t0.72755\t0.77959\t0.57997\t0.70862\t0.43364\t0.84483\t0.57241\t0.77709\t0.62359\t0.67308\t0.41446\t0.85311\t0.58307\t0.79735\n",
      "epoch=15-3mer-\t0.56082\t0.70472\t0.41011\t0.67613\t0.73331\t0.77941\t0.56883\t0.71207\t0.43033\t0.79655\t0.62759\t0.77705\t0.59038\t0.69532\t0.42284\t0.81078\t0.63759\t0.79764\n",
      "epoch=16-3mer-\t0.56511\t0.69551\t0.39117\t0.68189\t0.70913\t0.77278\t0.56399\t0.70000\t0.40009\t0.68966\t0.71034\t0.77655\t0.55006\t0.71884\t0.42075\t0.72803\t0.71424\t0.79800\n",
      "epoch=17-3mer-\t0.56488\t0.70395\t0.40849\t0.67728\t0.73062\t0.77349\t0.56829\t0.69655\t0.39325\t0.68276\t0.71034\t0.77721\t0.54518\t0.72119\t0.41886\t0.71392\t0.72482\t0.79819\n",
      "epoch=18-3mer-\t0.55740\t0.70223\t0.40450\t0.69417\t0.71028\t0.78160\t0.56382\t0.69655\t0.39386\t0.66552\t0.72759\t0.77592\t0.53814\t0.72204\t0.41073\t0.68890\t0.73861\t0.79847\n",
      "epoch=19-3mer-\t0.55757\t0.70146\t0.40360\t0.67229\t0.73062\t0.77971\t0.56194\t0.70345\t0.41211\t0.78276\t0.62414\t0.77794\t0.58456\t0.69959\t0.42790\t0.80885\t0.64496\t0.79894\n",
      "epoch=20-3mer-\t0.55626\t0.70414\t0.40880\t0.67920\t0.72909\t0.78286\t0.58000\t0.71034\t0.42762\t0.80000\t0.62069\t0.77782\t0.59020\t0.69660\t0.42995\t0.82168\t0.63406\t0.79876\n",
      "epoch=21-3mer-\t0.55498\t0.70376\t0.40767\t0.69033\t0.71719\t0.78427\t0.57210\t0.71034\t0.42709\t0.79655\t0.62414\t0.77748\t0.58989\t0.69596\t0.42875\t0.82104\t0.63342\t0.79911\n",
      "epoch=22-3mer-\t0.55065\t0.71335\t0.42681\t0.70223\t0.72448\t0.78876\t0.56619\t0.67759\t0.35810\t0.61379\t0.74138\t0.77556\t0.52916\t0.72910\t0.40548\t0.64080\t0.77325\t0.79728\n",
      "epoch=23-3mer-\t0.55126\t0.70932\t0.41909\t0.68649\t0.73216\t0.78756\t0.57783\t0.70172\t0.41538\t0.82069\t0.58276\t0.77762\t0.61013\t0.68227\t0.42358\t0.84734\t0.59974\t0.79947\n",
      "epoch=24-3mer-\t0.55587\t0.70510\t0.41022\t0.70107\t0.70913\t0.78397\t0.56269\t0.69483\t0.38977\t0.68276\t0.70690\t0.77966\t0.54331\t0.72375\t0.42412\t0.71713\t0.72707\t0.80035\n",
      "epoch=25-3mer-\t0.55370\t0.70587\t0.41261\t0.67345\t0.73830\t0.78523\t0.57061\t0.71034\t0.42608\t0.78966\t0.63103\t0.77993\t0.58564\t0.69895\t0.42792\t0.81078\t0.64304\t0.80024\n",
      "epoch=26-3mer-\t0.55896\t0.70434\t0.40876\t0.69378\t0.71489\t0.77946\t0.55323\t0.71207\t0.42658\t0.76552\t0.65862\t0.77994\t0.57364\t0.70451\t0.42674\t0.79153\t0.66100\t0.80034\n",
      "epoch=27-3mer-\t0.54992\t0.70741\t0.41514\t0.68764\t0.72717\t0.78806\t0.56673\t0.71552\t0.43499\t0.78276\t0.64828\t0.77881\t0.58155\t0.70237\t0.43303\t0.81142\t0.64785\t0.80021\n",
      "epoch=28-3mer-\t0.54935\t0.70779\t0.41567\t0.69724\t0.71834\t0.78940\t0.56091\t0.70000\t0.40216\t0.64828\t0.75172\t0.77847\t0.52818\t0.73102\t0.41762\t0.66709\t0.76299\t0.80092\n",
      "epoch=29-3mer-\t0.55036\t0.71086\t0.42183\t0.69916\t0.72256\t0.78776\t0.55575\t0.70862\t0.41997\t0.76552\t0.65172\t0.77988\t0.57293\t0.70473\t0.42793\t0.79346\t0.66036\t0.80149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../4-new-12w-0 were not used when initializing C_Bert_average_embedding: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing C_Bert_average_embedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing C_Bert_average_embedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Embedding end!!!--------------\n",
      "-----------------classifier-------------\n",
      "epoch=0-4mer-\t0.66680\t0.57982\t0.17616\t0.36838\t0.79125\t0.63493\t0.64348\t0.60862\t0.22489\t0.47931\t0.73793\t0.68747\t0.62935\t0.68591\t0.29318\t0.52854\t0.76459\t0.72637\n",
      "epoch=1-4mer-\t0.63327\t0.62682\t0.25835\t0.53185\t0.72180\t0.69644\t0.61892\t0.65000\t0.30998\t0.52414\t0.77586\t0.72199\t0.59050\t0.71670\t0.35387\t0.54715\t0.80148\t0.75954\n",
      "epoch=2-4mer-\t0.61052\t0.66443\t0.33239\t0.59171\t0.73715\t0.73161\t0.60896\t0.68448\t0.37138\t0.62759\t0.74138\t0.73389\t0.58853\t0.70301\t0.36630\t0.65170\t0.72867\t0.76903\n",
      "epoch=3-4mer-\t0.59873\t0.67978\t0.36113\t0.63315\t0.72640\t0.74654\t0.60442\t0.67759\t0.35528\t0.66552\t0.68966\t0.73832\t0.59141\t0.69403\t0.37915\t0.71841\t0.68185\t0.77320\n",
      "epoch=4-4mer-\t0.59500\t0.67441\t0.35057\t0.62433\t0.72448\t0.74433\t0.60194\t0.67414\t0.34863\t0.65172\t0.69655\t0.74224\t0.57897\t0.69852\t0.37766\t0.69852\t0.69852\t0.77697\n",
      "epoch=5-4mer-\t0.58514\t0.68438\t0.36947\t0.65349\t0.71527\t0.75667\t0.59900\t0.67069\t0.34678\t0.58276\t0.75862\t0.74816\t0.55802\t0.71777\t0.38911\t0.64849\t0.75241\t0.78112\n",
      "epoch=6-4mer-\t0.58355\t0.69033\t0.38126\t0.66232\t0.71834\t0.75761\t0.60507\t0.67414\t0.35680\t0.56552\t0.78276\t0.75008\t0.55053\t0.72140\t0.38874\t0.62989\t0.76716\t0.78264\n",
      "epoch=7-4mer-\t0.57907\t0.68803\t0.37774\t0.64083\t0.73523\t0.76275\t0.59077\t0.66897\t0.34823\t0.54828\t0.78966\t0.75094\t0.54305\t0.72418\t0.38734\t0.61129\t0.78063\t0.78420\n",
      "epoch=8-4mer-\t0.57694\t0.69513\t0.39046\t0.67882\t0.71144\t0.76257\t0.58400\t0.68448\t0.36976\t0.65172\t0.71724\t0.75131\t0.56006\t0.71028\t0.39707\t0.70173\t0.71456\t0.78486\n",
      "epoch=9-4mer-\t0.57129\t0.69302\t0.38694\t0.65886\t0.72717\t0.77007\t0.58221\t0.67931\t0.35870\t0.66897\t0.68966\t0.75394\t0.56794\t0.70515\t0.40231\t0.73380\t0.69083\t0.78662\n",
      "epoch=10-4mer-\t0.57338\t0.69398\t0.38830\t0.67268\t0.71527\t0.76610\t0.59183\t0.68448\t0.37013\t0.64483\t0.72414\t0.75636\t0.55693\t0.71520\t0.40775\t0.70943\t0.71809\t0.78773\n",
      "epoch=11-4mer-\t0.57802\t0.69282\t0.38600\t0.67153\t0.71412\t0.76141\t0.60257\t0.69828\t0.39914\t0.75517\t0.64138\t0.75566\t0.58995\t0.69083\t0.40792\t0.79282\t0.63983\t0.78898\n",
      "epoch=12-4mer-\t0.56929\t0.69992\t0.40026\t0.67728\t0.72256\t0.77135\t0.58969\t0.67931\t0.35931\t0.64828\t0.71034\t0.75667\t0.55890\t0.71285\t0.40975\t0.72354\t0.70750\t0.78946\n",
      "epoch=13-4mer-\t0.56609\t0.70069\t0.40155\t0.68611\t0.71527\t0.77410\t0.58366\t0.68103\t0.36285\t0.64828\t0.71379\t0.75616\t0.55604\t0.71477\t0.41064\t0.71841\t0.71296\t0.78958\n",
      "epoch=14-4mer-\t0.56794\t0.70069\t0.40208\t0.67114\t0.73024\t0.77260\t0.59593\t0.69483\t0.40189\t0.81724\t0.57241\t0.75748\t0.62321\t0.67501\t0.41354\t0.84670\t0.58916\t0.79044\n",
      "epoch=15-4mer-\t0.56945\t0.69896\t0.39798\t0.69071\t0.70721\t0.77012\t0.59508\t0.67069\t0.35274\t0.54483\t0.79655\t0.75528\t0.52972\t0.73060\t0.39898\t0.61257\t0.78961\t0.78951\n",
      "epoch=16-4mer-\t0.56507\t0.70568\t0.41150\t0.69263\t0.71873\t0.77417\t0.59251\t0.69138\t0.38627\t0.75862\t0.62414\t0.75945\t0.58810\t0.69232\t0.40881\t0.79025\t0.64336\t0.79117\n",
      "epoch=17-4mer-\t0.56601\t0.70280\t0.40667\t0.66654\t0.73906\t0.77292\t0.59059\t0.67759\t0.35672\t0.63103\t0.72414\t0.75668\t0.55166\t0.71563\t0.40741\t0.70686\t0.72001\t0.78973\n",
      "epoch=18-4mer-\t0.56098\t0.70395\t0.40801\t0.69263\t0.71527\t0.77993\t0.58398\t0.68448\t0.36946\t0.65862\t0.71034\t0.76075\t0.55603\t0.71306\t0.41346\t0.73188\t0.70366\t0.79173\n",
      "epoch=19-4mer-\t0.56036\t0.71144\t0.42328\t0.68956\t0.73331\t0.78013\t0.59298\t0.69655\t0.39583\t0.75517\t0.63793\t0.75977\t0.58417\t0.69403\t0.41123\t0.79025\t0.64593\t0.79113\n",
      "epoch=20-4mer-\t0.55726\t0.71124\t0.42282\t0.69148\t0.73101\t0.78299\t0.58339\t0.68621\t0.37392\t0.73103\t0.64138\t0.76001\t0.57606\t0.69981\t0.41295\t0.77614\t0.66164\t0.79163\n",
      "epoch=21-4mer-\t0.56083\t0.70280\t0.40620\t0.67575\t0.72985\t0.77881\t0.59545\t0.69655\t0.39908\t0.78276\t0.61034\t0.76237\t0.59452\t0.69019\t0.41476\t0.80885\t0.63085\t0.79237\n",
      "epoch=22-4mer-\t0.56318\t0.69800\t0.39658\t0.67114\t0.72487\t0.77563\t0.60181\t0.70000\t0.41219\t0.82069\t0.57931\t0.76124\t0.61156\t0.67821\t0.41070\t0.83323\t0.60071\t0.79243\n",
      "epoch=23-4mer-\t0.56233\t0.69570\t0.39216\t0.66462\t0.72678\t0.77621\t0.61187\t0.66552\t0.35669\t0.47931\t0.85172\t0.76159\t0.51482\t0.73167\t0.37520\t0.52213\t0.83643\t0.79267\n",
      "epoch=24-4mer-\t0.56009\t0.70779\t0.41573\t0.69417\t0.72141\t0.77884\t0.58361\t0.68966\t0.37932\t0.69310\t0.68621\t0.76361\t0.56408\t0.70836\t0.41775\t0.75882\t0.68313\t0.79276\n",
      "epoch=25-4mer-\t0.56271\t0.69666\t0.39419\t0.66347\t0.72985\t0.77807\t0.58720\t0.67759\t0.37498\t0.51724\t0.83793\t0.76642\t0.51696\t0.73209\t0.38507\t0.55613\t0.82008\t0.79370\n",
      "epoch=26-4mer-\t0.56017\t0.70472\t0.40954\t0.69340\t0.71604\t0.77957\t0.60271\t0.69310\t0.40532\t0.84483\t0.54138\t0.76414\t0.63748\t0.66581\t0.41279\t0.86722\t0.56511\t0.79309\n",
      "epoch=27-4mer-\t0.55881\t0.70721\t0.41462\t0.69186\t0.72256\t0.78121\t0.57279\t0.68276\t0.36941\t0.61034\t0.75517\t0.76357\t0.53934\t0.72675\t0.41576\t0.68185\t0.74920\t0.79263\n",
      "epoch=28-4mer-\t0.55481\t0.70856\t0.41768\t0.68266\t0.73446\t0.78482\t0.57465\t0.68103\t0.36780\t0.59310\t0.76897\t0.76554\t0.53411\t0.73145\t0.41723\t0.66389\t0.76523\t0.79371\n",
      "epoch=29-4mer-\t0.55724\t0.70069\t0.40163\t0.68304\t0.71834\t0.78062\t0.58747\t0.66034\t0.33695\t0.50690\t0.81379\t0.76516\t0.51995\t0.73273\t0.39279\t0.57922\t0.80949\t0.79331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../5-new-12w-0 were not used when initializing C_Bert_average_embedding: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing C_Bert_average_embedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing C_Bert_average_embedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Embedding end!!!--------------\n",
      "-----------------classifier-------------\n",
      "epoch=0-5mer-\t0.67537\t0.55219\t0.12263\t0.28972\t0.81466\t0.60304\t0.66023\t0.62931\t0.25930\t0.66552\t0.59310\t0.67685\t0.68119\t0.64058\t0.27632\t0.66325\t0.62925\t0.69623\n",
      "epoch=1-5mer-\t0.64869\t0.61090\t0.23245\t0.46124\t0.76055\t0.67592\t0.63950\t0.61552\t0.24932\t0.42759\t0.80345\t0.69973\t0.61916\t0.68890\t0.26703\t0.43489\t0.81591\t0.71900\n",
      "epoch=2-5mer-\t0.63802\t0.62183\t0.25160\t0.49731\t0.74635\t0.68705\t0.64175\t0.64828\t0.30983\t0.79310\t0.50345\t0.71624\t0.68086\t0.61492\t0.32389\t0.82745\t0.50866\t0.73307\n",
      "epoch=3-5mer-\t0.62765\t0.64256\t0.28727\t0.58135\t0.70376\t0.70500\t0.63201\t0.60172\t0.23492\t0.35172\t0.85172\t0.72401\t0.57777\t0.70130\t0.27483\t0.37588\t0.86402\t0.74044\n",
      "epoch=4-5mer-\t0.61744\t0.65004\t0.30379\t0.57214\t0.72794\t0.71999\t0.61882\t0.64828\t0.31768\t0.46897\t0.82759\t0.73068\t0.57527\t0.70772\t0.31064\t0.45799\t0.83258\t0.74763\n",
      "epoch=5-5mer-\t0.61279\t0.65196\t0.30720\t0.57905\t0.72487\t0.72281\t0.61028\t0.66897\t0.33952\t0.62069\t0.71724\t0.73765\t0.59841\t0.69211\t0.34777\t0.64849\t0.71392\t0.75480\n",
      "epoch=6-5mer-\t0.60613\t0.67038\t0.34329\t0.60975\t0.73101\t0.73511\t0.61087\t0.66552\t0.33111\t0.65517\t0.67586\t0.74011\t0.60169\t0.68505\t0.35163\t0.68505\t0.68505\t0.75724\n",
      "epoch=7-5mer-\t0.59913\t0.67594\t0.35377\t0.62433\t0.72755\t0.74242\t0.59753\t0.66897\t0.34226\t0.58966\t0.74828\t0.74536\t0.57905\t0.70772\t0.36290\t0.62155\t0.75080\t0.76372\n",
      "epoch=8-5mer-\t0.59900\t0.67306\t0.34823\t0.61819\t0.72794\t0.74048\t0.59844\t0.67241\t0.34722\t0.61379\t0.73103\t0.74749\t0.57913\t0.70473\t0.36689\t0.64593\t0.73412\t0.76561\n",
      "epoch=9-5mer-\t0.59904\t0.66539\t0.33299\t0.60783\t0.72295\t0.73714\t0.61720\t0.68966\t0.39230\t0.81724\t0.56207\t0.75015\t0.64319\t0.65213\t0.37139\t0.82489\t0.56575\t0.76856\n",
      "epoch=10-5mer-\t0.59305\t0.67095\t0.34238\t0.64467\t0.69724\t0.74541\t0.59179\t0.66034\t0.32803\t0.55517\t0.76552\t0.75155\t0.56156\t0.71477\t0.36685\t0.59846\t0.77293\t0.77030\n",
      "epoch=11-5mer-\t0.59277\t0.66865\t0.33925\t0.61512\t0.72218\t0.74376\t0.59774\t0.67931\t0.35862\t0.67931\t0.67931\t0.75289\t0.58574\t0.69510\t0.37590\t0.70686\t0.68922\t0.77202\n",
      "epoch=12-5mer-\t0.58876\t0.67594\t0.35374\t0.62471\t0.72717\t0.74994\t0.62139\t0.68621\t0.40374\t0.87931\t0.49310\t0.75473\t0.67929\t0.63182\t0.38085\t0.88647\t0.50449\t0.77331\n",
      "epoch=13-5mer-\t0.58377\t0.68381\t0.36900\t0.64045\t0.72717\t0.75616\t0.59088\t0.68966\t0.38164\t0.74483\t0.63448\t0.75622\t0.60178\t0.68121\t0.37982\t0.76203\t0.64080\t0.77372\n",
      "epoch=14-5mer-\t0.58396\t0.68841\t0.37763\t0.65579\t0.72103\t0.75730\t0.59522\t0.67759\t0.35650\t0.72069\t0.63448\t0.75641\t0.60262\t0.68313\t0.38674\t0.77101\t0.63919\t0.77666\n",
      "epoch=15-5mer-\t0.58385\t0.68630\t0.37365\t0.64889\t0.72371\t0.75521\t0.57998\t0.68793\t0.37968\t0.75862\t0.61724\t0.75861\t0.60708\t0.67693\t0.38320\t0.78191\t0.62444\t0.77705\n",
      "epoch=16-5mer-\t0.58213\t0.68170\t0.36448\t0.64313\t0.72026\t0.75690\t0.59005\t0.67241\t0.34695\t0.61724\t0.72759\t0.75948\t0.56165\t0.71178\t0.38446\t0.66260\t0.73637\t0.77857\n",
      "epoch=17-5mer-\t0.58771\t0.67498\t0.35143\t0.62932\t0.72064\t0.74831\t0.60703\t0.69655\t0.40983\t0.83793\t0.55517\t0.75976\t0.63867\t0.66025\t0.39653\t0.85119\t0.56479\t0.77945\n",
      "epoch=18-5mer-\t0.58496\t0.68227\t0.36538\t0.64850\t0.71604\t0.75264\t0.58420\t0.67414\t0.34938\t0.63448\t0.71379\t0.76119\t0.56588\t0.70665\t0.38461\t0.68441\t0.71777\t0.77981\n",
      "epoch=19-5mer-\t0.57537\t0.69091\t0.38249\t0.66117\t0.72064\t0.76487\t0.58170\t0.67586\t0.35799\t0.58276\t0.76897\t0.76243\t0.54982\t0.71841\t0.38390\t0.63053\t0.76235\t0.78087\n",
      "epoch=20-5mer-\t0.57925\t0.68035\t0.36161\t0.64505\t0.71566\t0.75815\t0.57838\t0.67586\t0.35584\t0.60000\t0.75172\t0.76304\t0.55141\t0.71948\t0.39059\t0.64464\t0.75690\t0.78110\n",
      "epoch=21-5mer-\t0.57809\t0.69033\t0.38200\t0.64850\t0.73216\t0.75996\t0.59439\t0.69138\t0.39037\t0.78966\t0.59310\t0.76328\t0.61384\t0.67522\t0.39903\t0.81847\t0.60359\t0.78180\n",
      "epoch=22-5mer-\t0.57462\t0.68688\t0.37493\t0.64735\t0.72640\t0.76360\t0.58618\t0.69828\t0.40690\t0.81034\t0.58621\t0.76441\t0.62263\t0.67137\t0.40118\t0.83258\t0.59076\t0.78299\n",
      "epoch=23-5mer-\t0.57784\t0.68649\t0.37388\t0.65196\t0.72103\t0.76038\t0.58641\t0.69138\t0.38704\t0.76552\t0.61724\t0.76499\t0.60111\t0.68399\t0.39740\t0.79089\t0.63053\t0.78350\n",
      "epoch=24-5mer-\t0.57655\t0.69206\t0.38430\t0.67652\t0.70760\t0.76304\t0.57395\t0.67931\t0.36174\t0.61379\t0.74483\t0.76616\t0.55230\t0.71777\t0.39440\t0.66325\t0.74503\t0.78364\n",
      "epoch=25-5mer-\t0.57411\t0.69071\t0.38300\t0.64543\t0.73599\t0.76621\t0.56677\t0.67759\t0.35523\t0.66897\t0.68621\t0.76640\t0.56996\t0.70451\t0.39440\t0.71713\t0.69820\t0.78410\n",
      "epoch=26-5mer-\t0.57299\t0.68995\t0.38021\t0.66961\t0.71028\t0.76673\t0.58347\t0.67931\t0.36111\t0.62069\t0.73793\t0.76663\t0.55472\t0.71563\t0.39471\t0.67351\t0.73669\t0.78406\n",
      "epoch=27-5mer-\t0.58470\t0.67709\t0.35528\t0.63776\t0.71642\t0.75230\t0.58844\t0.67069\t0.36839\t0.48276\t0.85862\t0.76727\t0.52338\t0.72996\t0.36600\t0.49904\t0.84541\t0.78516\n",
      "epoch=28-5mer-\t0.57347\t0.68918\t0.37924\t0.65503\t0.72333\t0.76389\t0.57625\t0.68448\t0.38270\t0.55172\t0.81724\t0.76839\t0.53372\t0.72803\t0.38515\t0.58242\t0.80083\t0.78551\n",
      "epoch=29-5mer-\t0.57087\t0.69570\t0.39218\t0.66424\t0.72717\t0.76908\t0.57363\t0.67241\t0.34925\t0.59310\t0.75172\t0.76738\t0.54751\t0.72226\t0.40016\t0.65876\t0.75401\t0.78457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../6-new-12w-0 were not used when initializing C_Bert_average_embedding: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing C_Bert_average_embedding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing C_Bert_average_embedding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Embedding end!!!--------------\n",
      "-----------------classifier-------------\n",
      "epoch=0-6mer-\t0.68947\t0.53089\t0.07306\t0.26401\t0.79777\t0.55463\t0.66801\t0.60862\t0.22578\t0.47241\t0.74483\t0.65509\t0.68233\t0.65683\t0.22182\t0.46953\t0.75048\t0.68284\n",
      "epoch=1-6mer-\t0.66011\t0.59210\t0.20449\t0.37490\t0.80929\t0.65685\t0.65456\t0.56724\t0.16977\t0.26207\t0.87241\t0.68502\t0.62893\t0.69040\t0.22303\t0.27582\t0.89769\t0.71555\n",
      "epoch=2-6mer-\t0.64925\t0.60035\t0.21440\t0.42441\t0.77629\t0.67408\t0.64269\t0.62069\t0.24476\t0.53793\t0.70345\t0.69486\t0.63724\t0.68334\t0.31235\t0.59269\t0.72867\t0.72717\n",
      "epoch=3-6mer-\t0.64145\t0.61972\t0.24657\t0.50038\t0.73906\t0.68409\t0.62854\t0.61897\t0.25314\t0.44828\t0.78966\t0.70812\t0.60671\t0.70344\t0.31486\t0.49968\t0.80532\t0.73748\n",
      "epoch=4-6mer-\t0.62499\t0.64524\t0.29772\t0.53569\t0.75480\t0.71383\t0.62516\t0.66552\t0.33607\t0.75172\t0.57931\t0.71788\t0.64976\t0.65277\t0.34116\t0.76459\t0.59686\t0.74409\n",
      "epoch=5-6mer-\t0.62300\t0.64390\t0.29207\t0.55871\t0.72909\t0.71416\t0.66008\t0.64655\t0.35240\t0.92414\t0.36897\t0.72189\t0.75001\t0.56468\t0.31583\t0.91341\t0.39031\t0.74708\n",
      "epoch=6-6mer-\t0.62557\t0.63699\t0.27613\t0.57483\t0.69916\t0.70284\t0.61287\t0.67069\t0.34335\t0.72414\t0.61724\t0.72687\t0.62863\t0.67073\t0.35744\t0.74599\t0.63310\t0.75210\n",
      "epoch=7-6mer-\t0.61051\t0.66193\t0.32722\t0.59056\t0.73331\t0.72960\t0.61968\t0.68276\t0.37950\t0.81724\t0.54828\t0.73250\t0.65931\t0.63994\t0.35349\t0.82168\t0.54907\t0.75628\n",
      "epoch=8-6mer-\t0.60426\t0.66846\t0.33866\t0.61781\t0.71911\t0.73580\t0.61841\t0.59655\t0.23028\t0.32414\t0.86897\t0.73718\t0.55391\t0.71499\t0.30522\t0.37396\t0.88550\t0.75915\n",
      "epoch=9-6mer-\t0.61594\t0.64140\t0.28699\t0.55641\t0.72640\t0.71734\t0.60945\t0.67586\t0.35847\t0.77241\t0.57931\t0.73887\t0.63400\t0.66453\t0.37109\t0.79217\t0.60071\t0.76118\n",
      "epoch=10-6mer-\t0.60592\t0.66999\t0.34078\t0.63584\t0.70414\t0.73197\t0.59605\t0.67759\t0.35578\t0.64828\t0.70690\t0.74203\t0.58948\t0.70109\t0.36939\t0.66774\t0.71777\t0.76336\n",
      "epoch=11-6mer-\t0.60283\t0.66539\t0.33317\t0.60553\t0.72525\t0.73467\t0.59862\t0.67759\t0.35779\t0.73793\t0.61724\t0.74403\t0.61283\t0.68484\t0.38265\t0.75690\t0.64881\t0.76501\n",
      "epoch=12-6mer-\t0.59616\t0.67345\t0.34800\t0.63354\t0.71335\t0.74494\t0.60422\t0.62759\t0.27842\t0.42759\t0.82759\t0.74570\t0.54921\t0.71969\t0.33697\t0.46632\t0.84638\t0.76662\n",
      "epoch=13-6mer-\t0.59402\t0.67575\t0.35363\t0.62087\t0.73062\t0.74652\t0.59871\t0.68103\t0.36389\t0.73103\t0.63103\t0.74835\t0.60888\t0.68399\t0.38113\t0.75625\t0.64785\t0.76858\n",
      "epoch=14-6mer-\t0.59407\t0.67364\t0.34898\t0.62433\t0.72295\t0.74555\t0.59028\t0.67414\t0.35115\t0.61034\t0.73793\t0.74839\t0.56971\t0.70815\t0.36338\t0.62091\t0.75176\t0.76911\n",
      "epoch=15-6mer-\t0.59594\t0.66942\t0.34015\t0.62548\t0.71335\t0.74085\t0.59733\t0.63793\t0.29814\t0.44828\t0.82759\t0.74899\t0.54423\t0.72247\t0.35021\t0.49583\t0.83579\t0.77031\n",
      "epoch=16-6mer-\t0.59541\t0.67210\t0.34685\t0.61051\t0.73369\t0.74276\t0.62528\t0.66897\t0.37520\t0.88621\t0.45172\t0.75283\t0.69977\t0.61899\t0.37136\t0.89673\t0.48012\t0.77183\n",
      "epoch=17-6mer-\t0.59123\t0.67920\t0.35946\t0.64083\t0.71757\t0.74707\t0.58668\t0.67241\t0.34582\t0.63448\t0.71034\t0.75332\t0.57308\t0.70922\t0.38034\t0.66260\t0.73252\t0.77272\n",
      "epoch=18-6mer-\t0.59349\t0.66999\t0.34136\t0.62510\t0.71489\t0.74486\t0.61208\t0.61034\t0.27767\t0.30690\t0.91379\t0.75599\t0.52789\t0.71820\t0.30316\t0.31559\t0.91950\t0.77373\n",
      "epoch=19-6mer-\t0.59482\t0.67345\t0.34837\t0.62740\t0.71949\t0.74417\t0.62836\t0.66552\t0.37638\t0.90345\t0.42759\t0.75460\t0.71349\t0.61022\t0.36646\t0.90571\t0.46248\t0.77239\n",
      "epoch=20-6mer-\t0.59462\t0.67229\t0.34551\t0.63584\t0.70875\t0.74220\t0.58114\t0.67586\t0.35664\t0.59310\t0.75862\t0.75642\t0.55795\t0.71371\t0.37112\t0.61642\t0.76235\t0.77408\n",
      "epoch=21-6mer-\t0.58095\t0.67843\t0.35782\t0.64198\t0.71489\t0.75877\t0.57312\t0.67759\t0.35993\t0.59655\t0.75862\t0.75779\t0.55587\t0.71542\t0.37489\t0.61899\t0.76363\t0.77566\n",
      "epoch=22-6mer-\t0.58359\t0.68400\t0.36955\t0.63814\t0.72985\t0.75646\t0.59422\t0.62931\t0.28767\t0.41034\t0.84828\t0.75785\t0.53294\t0.72055\t0.33743\t0.46119\t0.85022\t0.77586\n",
      "epoch=23-6mer-\t0.59000\t0.67325\t0.34843\t0.62087\t0.72563\t0.74812\t0.59513\t0.63103\t0.29318\t0.40690\t0.85517\t0.75795\t0.53199\t0.72226\t0.34020\t0.45799\t0.85439\t0.77602\n",
      "epoch=24-6mer-\t0.58455\t0.68361\t0.36791\t0.65311\t0.71412\t0.75515\t0.57347\t0.68448\t0.37271\t0.61379\t0.75517\t0.75826\t0.55763\t0.71328\t0.37396\t0.62668\t0.75657\t0.77576\n",
      "epoch=25-6mer-\t0.58887\t0.67575\t0.35379\t0.61896\t0.73254\t0.74884\t0.58321\t0.68103\t0.36243\t0.70345\t0.65862\t0.76010\t0.58698\t0.69810\t0.39122\t0.73252\t0.68089\t0.77632\n",
      "epoch=26-6mer-\t0.59562\t0.68016\t0.36032\t0.67997\t0.68035\t0.74263\t0.57506\t0.68793\t0.37750\t0.64138\t0.73448\t0.76008\t0.56530\t0.71392\t0.39244\t0.67479\t0.73348\t0.77719\n",
      "epoch=27-6mer-\t0.58339\t0.67939\t0.36120\t0.62164\t0.73715\t0.75530\t0.58776\t0.69310\t0.38858\t0.74828\t0.63793\t0.76088\t0.59762\t0.69403\t0.40201\t0.77037\t0.65587\t0.77770\n",
      "epoch=28-6mer-\t0.58129\t0.68189\t0.36412\t0.66002\t0.70376\t0.75766\t0.57821\t0.68793\t0.37651\t0.65862\t0.71724\t0.76121\t0.56928\t0.71157\t0.39432\t0.68954\t0.72258\t0.77795\n",
      "epoch=29-6mer-\t0.57810\t0.68438\t0.37025\t0.63968\t0.72909\t0.75972\t0.58148\t0.67069\t0.35340\t0.54138\t0.80000\t0.76111\t0.54030\t0.71734\t0.35779\t0.55613\t0.79795\t0.77820\n",
      "integrate-NHEK-ind_acc=0.70152; ind_mcc=0.41339; ind_sn=0.77165; ind_sp=0.66645; ind_AUC=0.79283\n"
     ]
    }
   ],
   "source": [
    "# average [3,4,5,6]\n",
    "# 添加L2-正则化 average\n",
    "# cell_lines = [\"GM12878\", \"HUVEC\", \"HSMM\", \"HEK293\", \"HMEC\", \"K562\", \"NHEK\", \"NHLF\"]\n",
    "cell_lines = [\"NHEK\"]\n",
    "\n",
    "final_filename = \"./result/record_NHEK.txt\"\n",
    "final_log = open(final_filename, \"w\")\n",
    "content = \"state\\ttrain-loss\\tacc\\tmcc\\tsn\\tsp\\tauc\\ttest-loss\\tacc\\tmcc\\tsn\\tsp\\tauc\\tind-loss\\tacc\\tmcc\\tsn\\tsp\\tauc\\n\"\n",
    "final_log.write(content)\n",
    "final_log.flush()\n",
    "\n",
    "for cell_line in cell_lines:\n",
    "    filename = \"./result/{}.txt\".format(cell_line)\n",
    "    file_log = open(filename, \"w\")\n",
    "    content = \"state\\ttrain-loss\\tacc\\tmcc\\tsn\\tsp\\tauc\\ttest-loss\\tacc\\tmcc\\tsn\\tsp\\tauc\\tind-loss\\tacc\\tmcc\\tsn\\tsp\\tauc\\n\"\n",
    "    file_log.write(content)\n",
    "    file_log.flush()\n",
    "\n",
    "    mers = [3, 4, 5, 6]\n",
    "    test_real_labels_list, test_pre_labels_list = [], []\n",
    "\n",
    "    for i in range(len(mers)):\n",
    "        mer = mers[i]\n",
    "        \n",
    "        args.model_path = \"../../DNA-BERT/{}-new-12w-0\".format(mer)\n",
    "        \n",
    "        tokenizer = BertTokenizer.from_pretrained(args.model_path)\n",
    "        DNABert = C_Bert_average_embedding.from_pretrained(args.model_path).to(device)\n",
    "\n",
    "        args.ind_filename = \"/home/lijiahao/project/Enhancer/Enhancer-IF/dataset/process/test-dataset(all-length)/{}-test-{}mer.txt\".format(cell_line, mer)\n",
    "        args.tra_filename = \"/home/lijiahao/project/Enhancer/Enhancer-IF/dataset/process/train-dataset(all-length)/{}-train-{}mer.txt\".format(cell_line, mer)\n",
    "        \n",
    "        tra_df_raw = pd.read_csv(args.tra_filename, sep=\"\\t\",header=None,names=[\"text\",\"label\"]) \n",
    "        ind_df_raw = pd.read_csv(args.ind_filename, sep=\"\\t\",header=None,names=[\"text\",\"label\"]) \n",
    "        tra_set, val_set = train_test_split(tra_df_raw, stratify=tra_df_raw['label'], test_size=0.1, random_state=42)\n",
    "\n",
    "        tra_text, tra_labels = list(tra_set[\"text\"]), list(tra_set[\"label\"])\n",
    "        val_text, val_labels = list(val_set[\"text\"]), list(val_set[\"label\"])\n",
    "        ind_text, ind_labels = list(ind_df_raw[\"text\"]), list(ind_df_raw[\"label\"])\n",
    "\n",
    "        tra_features = embedding(tra_text, DNABert, tokenizer)\n",
    "        val_features = embedding(val_text, DNABert, tokenizer)\n",
    "        ind_features = embedding(ind_text, DNABert, tokenizer)    \n",
    "\n",
    "        print(\"----------Embedding end!!!--------------\")\n",
    "        tra_dataset = NewDataset_classifier(tra_features, tra_labels)\n",
    "        tra_dataloader = DataLoader(tra_dataset, batch_size = args.batch_size, shuffle = True)\n",
    "\n",
    "        val_dataset = NewDataset_classifier(val_features, val_labels)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size = args.batch_size, shuffle = True)\n",
    "\n",
    "        ind_dataset = NewDataset_classifier(ind_features, ind_labels)\n",
    "        ind_dataloader = DataLoader(ind_dataset, batch_size = args.batch_size, shuffle = False)\n",
    "\n",
    "        print(\"-----------------classifier-------------\")\n",
    "        classifier_model = Enhancer_classifier_128().to(device)\n",
    "        epoches = args.epoches\n",
    "        learning_rate = args.learning_rate\n",
    "        optimizer = optim.Adam(classifier_model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08,)\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.98)    # 指数衰减损失函数\n",
    "        \n",
    "        acc_max = 0.0\n",
    "        temp_logits = []\n",
    "        content_temp = \"\"\n",
    "        for epoch in range(epoches):\n",
    "            tra_loss, tra_acc, tra_mcc, tra_sn, tra_sp, tra_auc = train_classifier(classifier_model, tra_dataloader, optimizer)\n",
    "            val_loss, val_acc, val_mcc, val_sn, val_sp, val_auc, _, _ = test_classifier(classifier_model, val_dataloader, optimizer)\n",
    "            ind_loss, ind_acc, ind_mcc, ind_sn, ind_sp, ind_auc, real_labels, pre_labels = test_classifier(classifier_model, ind_dataloader, optimizer)\n",
    "            content = \"epoch={}-{}mer-\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\t{:.5f}\\n\".format(epoch, mer,\\\n",
    "                tra_loss, tra_acc, tra_mcc, tra_sn, tra_sp, tra_auc, val_loss, val_acc, val_mcc, val_sn, val_sp, val_auc, ind_loss, ind_acc, ind_mcc, ind_sn, ind_sp, ind_auc)\n",
    "            \n",
    "            file_log.write(content)\n",
    "            file_log.flush()\n",
    "\n",
    "            print(content[:-1])\n",
    "\n",
    "            if np.abs(val_sn-val_sp) < 0.15 and acc_max < val_acc:\n",
    "                acc_max = val_acc\n",
    "                temp_logits = pre_labels\n",
    "                content_temp = content\n",
    "\n",
    "        test_pre_labels_list.append(temp_logits)\n",
    "        test_real_labels_list.append(real_labels)\n",
    "\n",
    "        final_log.write(\"{}-\".format(cell_line) + content_temp)\n",
    "        final_log.flush()\n",
    "\n",
    "    test_real_labels = np.mean(test_real_labels_list, axis=0)\n",
    "    test_pre_labels = np.mean(test_pre_labels_list, axis=0)\n",
    "\n",
    "    acc, mcc, sn, sp = evaluation_criterion(test_pre_labels, test_real_labels)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(test_real_labels, test_pre_labels)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    content = \"integrate-{}-ind_acc={:.5f}; ind_mcc={:.5f}; ind_sn={:.5f}; ind_sp={:.5f}; ind_AUC={:.5f}\\n\".format(cell_line, acc, mcc, sn, sp, roc_auc)\n",
    "    print(content, end=\"\")\n",
    "    final_log.write(content)\n",
    "    final_log.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_pytorch_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06233b56f961380e94bcca211189b77c55ac475925b9cac0cc39dad8759f75d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
